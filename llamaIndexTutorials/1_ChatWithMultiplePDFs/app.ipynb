{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG App using LLAMA Index Library having LLAMA3 LLM & nomic-embed-text as embedding model\n",
    "Main idea is to chat with multiple PDFs, where our LLM model & embedding model will be open source and running locally. So this app is suitable to chat on private data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"/home/akhil/personalProjects/resAssist/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.llms.ollama import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  75%|███████▌  | 9/12 [07:54<02:38, 52.72s/it]\n",
      "Parsing nodes: 100%|██████████| 39/39 [00:00<00:00, 772.79it/s]\n",
      "Generating embeddings: 100%|██████████| 64/64 [00:08<00:00,  7.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# Setup data directory for RAG with PDFs\n",
    "DATA_DIR = \"/home/akhil/personalProjects/resAssist/data_for_RAG/research_papers\"\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(DATA_DIR).load_data()\n",
    "\n",
    "# setup embedding model\n",
    "Settings.embed_model = OllamaEmbedding(model_name=\"nomic-embed-text\")\n",
    "\n",
    "# setup llm model\n",
    "Settings.llm = Ollama(model=\"llama3\", request_timeout=360.0)\n",
    "\n",
    "# create indexes for documents | uses text-embedding-ada-002 model to create vector embeddings (indexes)\n",
    "index = VectorStoreIndex.from_documents(documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Query Engine Rather than Chat Engine | As it is a Q/A Bot\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, it seems that there is no explicit discussion about the key challenges reported in research for the task of Automated Audio Captioning. However, some potential challenges that can be inferred from the text are:\n",
      "\n",
      "1. Balancing data: The authors mention the importance of balancing data when combining different datasets. This suggests that one challenge might be ensuring that each dataset contributes equally to the overall performance of the captioning system.\n",
      "2. Handling variability in audio and captions: The text mentions the use of different speech recognition systems, such as Whisper, which could introduce variability in the output. Similarly, the authors mention using different datasets with varying characteristics, which could also affect the performance of the captioning system.\n",
      "3. Overcoming limitations of language models: The authors discuss the use of pre-trained language models to improve the quality of captions. However, this might not be sufficient to overcome the challenges posed by the task of automated audio captioning.\n",
      "\n",
      "To improve automated audio captioning systems, some potential approaches could be:\n",
      "\n",
      "1. Developing more advanced speech recognition systems that can better handle variability in audio and captions.\n",
      "2. Using more diverse and balanced datasets to train the captioning system.\n",
      "3. Investigating the use of transfer learning or multitask learning to leverage knowledge from other related tasks, such as image captioning or natural language processing.\n",
      "4. Exploring the use of attention mechanisms or contextualized embeddings to better handle the nuances of audio data and improve the coherence of generated captions.\n",
      "\n",
      "It's worth noting that these suggestions are based on the context provided and might not be directly applicable to all research in automated audio captioning.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"\"\"\n",
    "    What are the key challenges reported in research for the task of Automated Audio Captioning? \n",
    "    Also let me know how to improve such captioning systems.\n",
    "    \"\"\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Multiple Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: Based on the provided context information, here is a\n",
      "brief explanation of the task of Automated Audio Captioning:\n",
      "Automated Audio Captioning (AAC) involves generating natural language\n",
      "descriptions or captions that accurately summarize and describe the\n",
      "content of an audio file. The goal is to develop a system that can\n",
      "automatically generate relevant and coherent text based on the audio\n",
      "input, which can be used for various applications such as search,\n",
      "retrieval, and indexing.\n",
      "______________________________________________________________________\n",
      "Source Node 1/4\n",
      "Node ID: f35136db-a7fd-45b8-bbb9-04c7ffe25170\n",
      "Similarity: 0.5272697155266008\n",
      "Text: Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer\n",
      "learningwith a unified text-to-text transformer,” J. Mach. Learn. Res.\n",
      ", vol. 21, no. 1, jan 2020. [33] A. van den Oord, Y . Li, and O.\n",
      "Vinyals, “Representation learning with contrastive predictive coding,”\n",
      "2018. [34] T. Pellegrini, I. Khalfaoui-Hassani, E. Labb ´e, and T.\n",
      "Masquelier,...\n",
      "______________________________________________________________________\n",
      "Source Node 2/4\n",
      "Node ID: 45ddf269-0d32-442b-ada2-69e73fc2c2c4\n",
      "Similarity: 0.5237680597057789\n",
      "Text: a is and speak man run engin by woman vehicl follow are in the\n",
      "an00.050.10.150.2Density .173 .106 .088 .060 .051 .024 .024 .018 .016\n",
      ".014 .014 .013 .012 .012 .010(a) Distribution of the unigrams in the\n",
      "candidate captions on AC-test with AC TE. a is and the in background\n",
      "speak while man are run peopl talk it then00.050.10.150.2Density .129\n",
      ".110 ....\n",
      "______________________________________________________________________\n",
      "Source Node 3/4\n",
      "Node ID: 27cf69f6-34a4-49e7-b0b1-160cdb1c8cf0\n",
      "Similarity: 0.49432430159328145\n",
      "Text: [12] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\n",
      "A. N. Gomez, L. u. Kaiser, and I. Polosukhin, “Attention is all you\n",
      "need,” in Advances in Neural Information Processing Systems , I.\n",
      "Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S.\n",
      "Vishwanathan, and R. Garnett, Eds., vol. 30. Curran Associates, Inc.,\n",
      "2017. [13] E. Kim, J...\n",
      "______________________________________________________________________\n",
      "Source Node 4/4\n",
      "Node ID: 137dc660-882f-4486-a712-3d23f33b299f\n",
      "Similarity: 0.4829994331040605\n",
      "Text: This tag replaces the <bos> token used at the sentence’s\n",
      "beginning and enables the model to generate an output closer to the\n",
      "desired writing style. We initially employed this method in our\n",
      "participation in the DCASE Challenge 2023 [49], where our best system\n",
      "achieved third place. In this study, we expanded on this approach by\n",
      "using more datasets...\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.indices.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.core.response.pprint_utils import pprint_response\n",
    "\n",
    "retreiver = VectorIndexRetriever(index=index, similarity_top_k=4)\n",
    "query_engine = RetrieverQueryEngine(retriever=retreiver)\n",
    "\n",
    "response = query_engine.query(\n",
    "    \"\"\"\n",
    "    Explain me the task of Automated Audio Captioning briefly.\n",
    "    \"\"\"\n",
    ")\n",
    "pprint_response(response, show_source=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: Based on the provided context, Automated Audio\n",
      "Captioning is a task that involves generating natural language\n",
      "captions to describe audio content, such as music or spoken words. The\n",
      "goal is to automatically generate accurate and informative captions\n",
      "for audio files, allowing users to easily identify and understand the\n",
      "content of the audio without having to listen to it in its entirety.\n",
      "______________________________________________________________________\n",
      "Source Node 1/2\n",
      "Node ID: f35136db-a7fd-45b8-bbb9-04c7ffe25170\n",
      "Similarity: 0.5272697155266008\n",
      "Text: Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer\n",
      "learningwith a unified text-to-text transformer,” J. Mach. Learn. Res.\n",
      ", vol. 21, no. 1, jan 2020. [33] A. van den Oord, Y . Li, and O.\n",
      "Vinyals, “Representation learning with contrastive predictive coding,”\n",
      "2018. [34] T. Pellegrini, I. Khalfaoui-Hassani, E. Labb ´e, and T.\n",
      "Masquelier,...\n",
      "______________________________________________________________________\n",
      "Source Node 2/2\n",
      "Node ID: 45ddf269-0d32-442b-ada2-69e73fc2c2c4\n",
      "Similarity: 0.5237680597057789\n",
      "Text: a is and speak man run engin by woman vehicl follow are in the\n",
      "an00.050.10.150.2Density .173 .106 .088 .060 .051 .024 .024 .018 .016\n",
      ".014 .014 .013 .012 .012 .010(a) Distribution of the unigrams in the\n",
      "candidate captions on AC-test with AC TE. a is and the in background\n",
      "speak while man are run peopl talk it then00.050.10.150.2Density .129\n",
      ".110 ....\n"
     ]
    }
   ],
   "source": [
    "# Applying a similarity threshold\n",
    "retreiver = VectorIndexRetriever(index=index, similarity_top_k=4)\n",
    "postprocessor = SimilarityPostprocessor(similarity_cutoff=0.5)\n",
    "query_engine = RetrieverQueryEngine(retriever=retreiver, node_postprocessors=[postprocessor])\n",
    "\n",
    "response = query_engine.query(\n",
    "    \"\"\"\n",
    "    Explain me the task of Automated Audio Captioning briefly.\n",
    "    \"\"\"\n",
    ")\n",
    "pprint_response(response, show_source=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistence Indexes\n",
    "Storing generated indexed presistently (on hard-disk) rather than on RAM and then applying RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, audio captioning refers to the task of generating a natural language description for an audio signal or a segment of an audio file. This task involves recognizing and describing the content, events, or actions present in the audio signal.\n",
      "\n",
      "The neural network architectures used for audio captioning are not explicitly stated in the given context, but based on the papers cited (e.g., [34], [36], [45]), it can be inferred that various types of neural networks have been employed for this task. Some possible architectures include:\n",
      "\n",
      "1. Convolutional Neural Networks (CNNs) or ConvNeXt models for audio classification and feature extraction.\n",
      "2. Recurrent Neural Networks (RNNs) or Long Short-Term Memory (LSTM) networks for sequence modeling and processing.\n",
      "3. Transformer-based models, such as the unified text-to-text transformer mentioned in [33], which could be used for audio captioning.\n",
      "4. MobileNetV2 architectures with inverted residuals and linear bottlenecks, as described in [36].\n",
      "5. Other neural network architectures that may have been employed for specific tasks or experiments within the papers cited.\n",
      "\n",
      "Keep in mind that this is an inference based on the provided context, and the actual neural network architectures used for audio captioning might vary depending on the specific research studies or applications.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import load_index_from_storage, StorageContext\n",
    "\n",
    "PERSIST_DIR = DATA_DIR + \"/RAGIndexesStorage\"\n",
    "\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    # load the documents and create the index\n",
    "    documents = SimpleDirectoryReader(DATA_DIR).load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    # store it for later\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    # load the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context)\n",
    "\n",
    "# either way we can now query the index\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is Audio Captioning and What type of neural network architectures are used?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
